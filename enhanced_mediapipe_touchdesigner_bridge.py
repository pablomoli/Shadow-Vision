Gochoprimo
gochoprimo
Online

little Trapz â€” 9/18/2025 2:28 PM
@Gochoprimo ^^
Gochoprimo â€” 9/18/2025 2:29 PM
Image
thanks he let me know
little Trapz â€” 9/18/2025 2:38 PM
are we using embedded searching with the CV or smthing else?
Gochoprimo â€” 9/18/2025 2:42 PM
Idk yet tbh
pablo
[KH]
 â€” 9/18/2025 2:49 PM
wym searching
pablo
[KH]
 â€” 9/21/2025 10:56 PM
I got camera feeds and filters to work on on the web
Gochoprimo â€” 9/22/2025 12:02 AM
is it slower than locally running it ?
pablo
[KH]
 â€” 9/22/2025 12:02 AM
itâ€™s not like on the internet itâ€™s just purely JS so itâ€™s lightweight
thatâ€™s what i mean mb
pablo
[KH]
 â€” 9/22/2025 10:49 PM
https://www.kaggle.com/datasets/vikranthkanumuru/naruto-hand-sign-dataset
Naruto Hand Sign Dataset
Image
Gochoprimo â€” 9/22/2025 10:49 PM
LOL its some indian dude
Gochoprimo â€” 9/23/2025 4:21 PM
this is google's thing for shellhacks
Image
https://weareinit.notion.site/Hacker-Guide-11c9f4e8ae4e81ce8f4eef9d9a8d701a
weareinit on Notion
Hacker Guide  | Notion
This guide contains all the information you need about this yearâ€™s ShellHacks. Please note that the guide will be gradually updated as we get closer to ShellHacks, so be sure to check back often!

Last updated: 09/20/2025
Gochoprimo
 pinned a message to this channel. See all pinned messages. â€” 9/23/2025 4:28 PM
Gochoprimo â€” 9/23/2025 4:28 PM
^pinned the hacker guide so we have an easy link to it
pablo
[KH]
 â€” 9/23/2025 10:15 PM
I can meet tn if we still need
Outerghost â€” 9/26/2025 3:04 PM
Attachment file type: acrobat
Pablo_Molina_Resume.pdf
112.94 KB
@little Trapz send resume pookie
little Trapz â€” 9/26/2025 3:08 PM
my b sent it to gaberial accidentally
Attachment file type: acrobat
Andres_Dominguez_Resume.pdf
200.60 KB
pablo
[KH]
 â€” 9/26/2025 3:08 PM
gaberial
little Trapz â€” 9/26/2025 3:26 PM
Attachment file type: acrobat
Andres_Dominguez_ResumeBW.pdf
201.62 KB
word sucks
Attachment file type: acrobat
Andres_Dominguez_ResumeBW.pdf
201.62 KB
pablo
[KH]
 â€” 9/26/2025 3:30 PM
https://github.com/pablomoli/Shadow-Vision
GitHub
pablomoli/Shadow-Vision
Contribute to pablomoli/Shadow-Vision development by creating an account on GitHub.
Contribute to pablomoli/Shadow-Vision development by creating an account on GitHub.
little Trapz â€” 9/26/2025 3:30 PM
pablo
 pinned a message to this channel. See all pinned messages. â€” 9/26/2025 3:30 PM
pablo
[KH]
 â€” 9/26/2025 3:31 PM
Gochoprimo â€” 9/26/2025 3:35 PM
Attachment file type: acrobat
Gabriel_Suarez_Resume (4).pdf
137.16 KB
Gochoprimo â€” 9/26/2025 3:43 PM
Attachment file type: acrobat
Gabriel_Suarez_Resume (6).pdf
468.94 KB
pablo
[KH]
 â€” 9/26/2025 4:40 PM
https://weareinit.notion.site/2429f4e8ae4e8093aabcc673bc3b06b5?v=2429f4e8ae4e811ba8ec000c5b111aed
weareinit on Notion
ShellHacks Schedule | Notion
View our ShellHacks 2025 Hacker Guide Schedule! To view events by day, please click the tabs to filter by 'All', 'Day', and 'Workshops/Eventsâ€™.
Please note that we will be updating the schedule in the coming days!
ShellHacks Schedule | Notion
pablo
[KH]
 â€” 9/26/2025 5:22 PM
https://link.springer.com/article/10.1007/s10055-023-00858-0
SpringerLink
Lightweight real-time hand segmentation leveraging MediaPipe landma...
Virtual Reality - Real-time hand segmentation is a key process in applications that require humanâ€“computer interaction, such as gesture recognition or augmented reality systems. However, the...
Lightweight real-time hand segmentation leveraging MediaPipe landma...
pablo
[KH]
 â€” 9/26/2025 5:41 PM
https://www.semanticscholar.org/paper/Ego2Hands%3A-A-Dataset-for-Egocentric-Two-hand-and-Lin-Martinez/39a5b7f5e3ebe220330b7d5f3ed3745e56c33350
Outerghost â€” 9/26/2025 5:44 PM
Gesture recognition
https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer/python

Image segment for masking:
https://ai.google.dev/edge/mediapipe/solutions/vision/image_segmenter
Google AI for Developers
Gesture recognition guide for Python  |  Google AI Edge  |  Goo...
Gesture recognition guide for Python  |  Google AI Edge  |  Goo...
Google AI for Developers
Image segmentation guide  |  Google AI Edge  |  Google AI for D...
Image segmentation guide  |  Google AI Edge  |  Google AI for D...
Outerghost â€” Yesterday at 2:44 AM
https://www.kaggle.com/groups/shellhacks/pending-invite/5AEEC9F8-84A8-4096-ACE4-BEEA8AE15CD5
pablo
[KH]
 â€” Yesterday at 4:00 AM
features to brain to label to obj to point cloud, use point cloud to interact with
Outerghost â€” Yesterday at 11:48 AM
LeoSadoun
little Trapz â€” Yesterday at 11:50 AM
https://huggingface.co/datasets/adominguez07/Animal-Puppet-Dataset/settings
Outerghost â€” Yesterday at 5:37 PM
check out what i have so far
http://localhost:8000/
Gochoprimo â€” Yesterday at 6:20 PM
python backend/cv_pipeline/advanced_realtime_inference.py
pablo
[KH]
 â€” Yesterday at 10:55 PM
https://github.com/torinmb/mediapipe-touchdesigner?tab=readme-ov-file
GitHub
GitHub - torinmb/mediapipe-touchdesigner: GPU Accelerated MediaPipe...
GPU Accelerated MediaPipe Plugin for TouchDesigner - torinmb/mediapipe-touchdesigner
GPU Accelerated MediaPipe Plugin for TouchDesigner - torinmb/mediapipe-touchdesigner
Gochoprimo â€” 12:11 AM
 ðŸ”§ Core Technologies & Libraries

  Computer Vision & AI

  - MediaPipe 0.10.21 - Google's real-time hand landmark detection (21 precise keypoints)
  - OpenCV (cv2) - Computer vision operations, camera capture, image processing
Expand
message.txt
5 KB
pablo
[KH]
 â€” 2:52 AM
Image
pablo
[KH]
 â€” 3:00 AM
Attachment file type: unknown
NewProject.12.toe
34.28 KB
Attachment file type: unknown
NewProject.toe
34.28 KB
pablo
[KH]
 â€” 5:21 AM
https://devpost.com/software/1070626/joins/cN4-HIHrlNKrMQv9MQCoqA

devpost team link
pablo
[KH]
 â€” 5:39 AM
use this
Outerghost â€” 8:05 AM
Attachment file type: unknown
NewProject(1).toe
37.45 KB
Outerghost â€” 8:27 AM
Attachment file type: unknown
bird.obj
137.44 KB
Attachment file type: unknown
cat.obj
74.93 KB
Attachment file type: unknown
deer.obj
103.92 KB
Attachment file type: unknown
dog.obj
88.02 KB
Attachment file type: unknown
Llama.obj
128.92 KB
Attachment file type: unknown
rabbit.obj
110.59 KB
Attachment file type: unknown
snail.obj
25.28 KB
Attachment file type: unknown
swan.obj
93.33 KB
little Trapz â€” 9:28 AM
Image
little Trapz â€” 10:29 AM
Image
little Trapz â€” 10:38 AM
#!/usr/bin/env python3
"""
Enhanced MediaPipe TouchDesigner Bridge with Full Landmark Streaming
Provides both gesture recognition AND raw landmark data for TouchDesigner.
Optimized for live demo performance with minimal latency.
"""
Expand
message.txt
30 KB
ï»¿
#!/usr/bin/env python3
"""
Enhanced MediaPipe TouchDesigner Bridge with Full Landmark Streaming
Provides both gesture recognition AND raw landmark data for TouchDesigner.
Optimized for live demo performance with minimal latency.
"""

import cv2
import numpy as np
import logging
import json
import time
import joblib
from pathlib import Path
from pythonosc import udp_client
from pythonosc.osc_message_builder import OscMessageBuilder
import sys

# Add backend to path
sys.path.append('backend')
sys.path.append('.')

from backend.data.mediapipe_extractor_real import RealMediaPipeExtractor, TwoHandDetection

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedTouchDesignerBridge:
    """Enhanced MediaPipe TouchDesigner bridge with full landmark streaming."""

    def __init__(self, td_ip="127.0.0.1", td_port=7000,
                 confidence_threshold=0.7, stability_duration=1.0,
                 stream_landmarks=True, stream_video=False,
                 label_port=7010):  # label_port retained for backward compatibility (dog port)
        """Initialize enhanced TouchDesigner bridge."""
        self.td_ip = td_ip
        self.td_port = td_port
        self.stream_landmarks = stream_landmarks
        self.stream_video = stream_video
        self.label_port = label_port  # legacy single-port (mapped to 'dog' by default)

        # OSC client for sending to TouchDesigner (full data)
        self.osc_client = udp_client.SimpleUDPClient(td_ip, td_port)
        # Additional OSC client for unified numeric label output (port 8000)
        try:
            self.numeric_label_port = 8000
            self.numeric_client = udp_client.SimpleUDPClient(td_ip, self.numeric_label_port)
        except Exception as e:
            logger.error(f"Failed to create numeric label OSC client on {td_ip}:8000: {e}")
            self.numeric_client = None
        # Per-animal secondary OSC clients (one port per label)
        # Mapping required by user: dog, cat, deer, swan, rabbit, bird, snail â†’ 7010-7016
        self.label_port_map = {
            'dog': 7010,
            'cat': 7011,
            'deer': 7012,
            'swan': 7013,
            'rabbit': 7014,
            'bird': 7015,
            'snail': 7016,
            'llama': 7017,
        }
        self.label_clients = {}
        for animal, port in self.label_port_map.items():
            try:
                self.label_clients[animal] = udp_client.SimpleUDPClient(td_ip, port)
            except Exception as e:
                logger.error(f"Failed to create label OSC client for {animal} on {td_ip}:{port}: {e}")
        # Backwards compatibility single client (points to 'dog' port) if needed elsewhere
        self.label_client = self.label_clients.get('dog', None)

        # Load MediaPipe model
        self.classes = ['bird', 'cat', 'llama', 'rabbit', 'deer', 'dog', 'snail', 'swan']
        self.load_mediapipe_model()

        # Initialize stable output buffer
        from mediapipe_touchdesigner_bridge import TwoHandStableOutputBuffer
        self.output_buffer = TwoHandStableOutputBuffer(confidence_threshold, stability_duration)

        # Performance tracking
        self.frame_count = 0
        self.predictions_sent = 0
        self.landmarks_sent = 0
        self.last_fps_time = time.time()
        self.fps = 0.0

        # TouchDesigner landmark format options
        self.landmark_format = "individual"  # "individual", "array", "both"
        # Toggle to get info-level logs for each label sent to its dedicated port
        self.verbose_label_logging = False

        # Init logging summary
        logger.info(f"Enhanced MediaPipe TouchDesigner bridge ready - {self.td_ip}:{self.td_port}")
        logger.info("Selected label ports:")
        for animal, port in self.label_port_map.items():
            logger.info(f"  {animal}: {self.td_ip}:{port}")
        logger.info(f"Landmark streaming: {self.stream_landmarks}, Video streaming: {self.stream_video}")
        logger.info(f"Stable output: {confidence_threshold:.0%} confidence, {stability_duration:.1f}s duration")

    def load_mediapipe_model(self):
        """Load the MediaPipe ML model."""
        try:
            models_dir = Path('models')
            self.model = joblib.load(models_dir / 'mediapipe_shadow_puppet_classifier.joblib')
            self.scaler = joblib.load(models_dir / 'mediapipe_scaler.joblib')
            self.feature_selector = joblib.load(models_dir / 'mediapipe_feature_selector.joblib')

            # Initialize MediaPipe extractor
            self.extractor = RealMediaPipeExtractor()

            logger.info("MediaPipe model loaded successfully")
            logger.info(f"Classes: {', '.join(self.classes)}")

        except Exception as e:
            logger.error(f"Failed to load MediaPipe model: {e}")
            raise

    def predict_single_hand(self, landmarks):
        """Predict gesture for a single hand."""
        try:
            if landmarks is None:
                return None, 0.0

            # Extract advanced features
            advanced_features = self.extractor.extract_advanced_features(landmarks)
            feature_vector = advanced_features.to_vector().reshape(1, -1)

            # Preprocessing
            scaled_features = self.scaler.transform(feature_vector)
            selected_features = self.feature_selector.transform(scaled_features)

            # Prediction
            prediction = self.model.predict(selected_features)[0]
            probabilities = self.model.predict_proba(selected_features)[0]
            confidence = probabilities.max()

            predicted_class = self.classes[prediction]
            return predicted_class, confidence

        except Exception as e:
            logger.error(f"Single hand prediction error: {e}")
            return None, 0.0

    def predict_both_hands(self, frame):
        """Predict gestures for both hands."""
        try:
            # Extract both hands
            two_hand_detection = self.extractor.extract_both_hands_from_frame(frame)

            left_animal = None
            left_confidence = 0.0
            right_animal = None
            right_confidence = 0.0

            # Predict for left hand
            if two_hand_detection.left_hand:
                left_animal, left_confidence = self.predict_single_hand(two_hand_detection.left_hand)

            # Predict for right hand
            if two_hand_detection.right_hand:
                right_animal, right_confidence = self.predict_single_hand(two_hand_detection.right_hand)

            return two_hand_detection, left_animal, left_confidence, right_animal, right_confidence

        except Exception as e:
            logger.error(f"Two-hand prediction error: {e}")
            return None, None, 0.0, None, 0.0

    def send_landmark_data_to_touchdesigner(self, two_hand_detection):
        """Send raw landmark data to TouchDesigner in optimized format."""
        if not self.stream_landmarks or not two_hand_detection:
            return

        try:
            # Send left hand landmarks
            if two_hand_detection.left_hand:
                self.send_hand_landmarks("left", two_hand_detection.left_hand)

            # Send right hand landmarks
            if two_hand_detection.right_hand:
                self.send_hand_landmarks("right", two_hand_detection.right_hand)

            # Send hand detection status
            self.osc_client.send_message("/landmarks/left/detected",
                                       1.0 if two_hand_detection.left_hand else 0.0)
            self.osc_client.send_message("/landmarks/right/detected",
                                       1.0 if two_hand_detection.right_hand else 0.0)

            self.landmarks_sent += 1

        except Exception as e:
            logger.error(f"Failed to send landmark data: {e}")

    def send_hand_landmarks(self, hand_side, landmarks):
        """Send individual hand landmarks in TouchDesigner-optimized format."""
        try:
            # Convert landmarks to list format
            landmark_array = landmarks.to_vector()  # 63 values (21 landmarks Ã— 3 coords)

            if self.landmark_format in ["individual", "both"]:
                # Send individual landmark coordinates (preferred for TouchDesigner arrays)
                for i in range(21):
                    idx = i * 3
                    x, y, z = landmark_array[idx], landmark_array[idx + 1], landmark_array[idx + 2]

                    self.osc_client.send_message(f"/landmarks/{hand_side}/{i}/x", float(x))
                    self.osc_client.send_message(f"/landmarks/{hand_side}/{i}/y", float(y))
                    self.osc_client.send_message(f"/landmarks/{hand_side}/{i}/z", float(z))

                # Send landmark names for TouchDesigner reference
                landmark_names = [
                    "wrist", "thumb_cmc", "thumb_mcp", "thumb_ip", "thumb_tip",
                    "index_mcp", "index_pip", "index_dip", "index_tip",
                    "middle_mcp", "middle_pip", "middle_dip", "middle_tip",
                    "ring_mcp", "ring_pip", "ring_dip", "ring_tip",
                    "pinky_mcp", "pinky_pip", "pinky_dip", "pinky_tip"
                ]

                for i, name in enumerate(landmark_names):
                    idx = i * 3
                    self.osc_client.send_message(f"/landmarks/{hand_side}/{name}/x", float(landmark_array[idx]))
                    self.osc_client.send_message(f"/landmarks/{hand_side}/{name}/y", float(landmark_array[idx + 1]))
                    self.osc_client.send_message(f"/landmarks/{hand_side}/{name}/z", float(landmark_array[idx + 2]))

            if self.landmark_format in ["array", "both"]:
                # Send as array for batch processing in TouchDesigner
                self.osc_client.send_message(f"/landmarks/{hand_side}/array", landmark_array.tolist())

            # Send derived features for advanced TouchDesigner control
            advanced_features = self.extractor.extract_advanced_features(landmarks)

            # Finger lengths (useful for gesture analysis in TD)
            finger_lengths = advanced_features.finger_lengths
            for i, length in enumerate(finger_lengths):
                self.osc_client.send_message(f"/landmarks/{hand_side}/finger_{i}/length", float(length))

            # Finger angles (useful for pose analysis in TD)
            finger_angles = advanced_features.finger_angles
            for i, angle in enumerate(finger_angles):
                self.osc_client.send_message(f"/landmarks/{hand_side}/finger_{i}/angle", float(angle))

            # Hand span and orientation
            self.osc_client.send_message(f"/landmarks/{hand_side}/hand_span_x", float(advanced_features.hand_span[0]))
            self.osc_client.send_message(f"/landmarks/{hand_side}/hand_span_y", float(advanced_features.hand_span[1]))
            self.osc_client.send_message(f"/landmarks/{hand_side}/orientation", float(advanced_features.hand_orientation))

            # Palm center (useful for object positioning in TD)
            self.osc_client.send_message(f"/landmarks/{hand_side}/palm_x", float(advanced_features.palm_center[0]))
            self.osc_client.send_message(f"/landmarks/{hand_side}/palm_y", float(advanced_features.palm_center[1]))
            self.osc_client.send_message(f"/landmarks/{hand_side}/palm_z", float(advanced_features.palm_center[2]))

        except Exception as e:
            logger.error(f"Failed to send {hand_side} hand landmarks: {e}")

    def send_gesture_to_touchdesigner(self, gesture, confidence, left_hand, right_hand, hand_positions=None):
        """Send gesture recognition results to TouchDesigner."""
        try:
            # Main gesture message (existing format)
            self.osc_client.send_message("/shadow_puppet/gesture", gesture)
            self.osc_client.send_message("/shadow_puppet/confidence", confidence)

            # Individual hand animals
            self.osc_client.send_message("/shadow_puppet/left_hand", left_hand if left_hand else "none")
            self.osc_client.send_message("/shadow_puppet/right_hand", right_hand if right_hand else "none")

            # Hand count
            hand_count = (1 if left_hand else 0) + (1 if right_hand else 0)
            self.osc_client.send_message("/shadow_puppet/hand_count", hand_count)

            # Additional data
            timestamp = time.time()
            self.osc_client.send_message("/shadow_puppet/timestamp", timestamp)

            # Hand positions if available
            if hand_positions:
                if hand_positions.get('left'):
                    self.osc_client.send_message("/shadow_puppet/left_x", hand_positions['left'][0])
                    self.osc_client.send_message("/shadow_puppet/left_y", hand_positions['left'][1])
                if hand_positions.get('right'):
                    self.osc_client.send_message("/shadow_puppet/right_x", hand_positions['right'][0])
                    self.osc_client.send_message("/shadow_puppet/right_y", hand_positions['right'][1])

            # Animal indices for TouchDesigner arrays
            left_index = self.classes.index(left_hand) if left_hand and left_hand in self.classes else -1
            right_index = self.classes.index(right_hand) if right_hand and right_hand in self.classes else -1
            self.osc_client.send_message("/shadow_puppet/left_index", left_index)
            self.osc_client.send_message("/shadow_puppet/right_index", right_index)

            # Status
            self.osc_client.send_message("/shadow_puppet/status", "confirmed")

            # Also send simplified confirmed label on secondary port
            self.send_selected_label(gesture)
            # Send numeric label value for the primary chosen gesture (first valid hand)
            self.send_numeric_label(left_hand, right_hand)

            self.predictions_sent += 1
            logger.info(f"Sent to TouchDesigner: {gesture} ({confidence:.1%}) - Predictions: {self.predictions_sent}, Landmarks: {self.landmarks_sent}")

        except Exception as e:
            logger.error(f"Failed to send gesture to TouchDesigner: {e}")

    def send_selected_label(self, label):
        """Send confirmed gesture's individual hand labels to their dedicated ports.

        Accepts combined gesture strings like 'L:dog+R:cat' or single-hand like 'L:dog'.
    For each recognized animal (dog, cat, deer, swan, rabbit, bird, snail, llama) we send:
           OSC Path: /shadow_puppet/selected_label  (string payload of the animal)
           Port: animal-specific (7010-7016)
    Llama uses port 7017.

        If no valid animal found, nothing is sent.
        """
        try:
            if not label:
                return

            # Split potential multi-hand gesture
            parts = label.split('+') if '+' in label else [label]
            sent_any = False
            for part in parts:
                # Expect format 'L:dog' or 'R:cat'; fall back to raw animal if no ':'
                if ':' in part:
                    _, animal = part.split(':', 1)
                else:
                    animal = part
                animal_key = animal.strip().lower()
                if animal_key in self.label_clients:
                    client = self.label_clients[animal_key]
                    try:
                        client.send_message("/shadow_puppet/selected_label", animal_key)
                        if self.verbose_label_logging:
                            logger.info(f"Label '{animal_key}' â†’ port {self.label_port_map[animal_key]} /shadow_puppet/selected_label")
                        else:
                            logger.debug(f"Sent selected label '{animal_key}' to {self.td_ip}:{self.label_port_map[animal_key]}")
                        sent_any = True
                    except Exception as send_err:
                        logger.error(
                            f"Error sending label '{animal_key}' to port {self.label_port_map[animal_key]}: {send_err}"
                        )
                else:
                    logger.debug(f"Ignoring non-mapped gesture segment '{part}' from '{label}'")

            if not sent_any:
                logger.debug(f"No mapped labels found in gesture '{label}' - nothing sent")
        except Exception as e:
            logger.error(f"Failed to process/send selected label(s) from '{label}': {e}")

    def send_numeric_label(self, left_hand, right_hand):
        """Send a single numeric value representing the chosen animal to port 8000.

        Selection priority: left hand if valid, else right hand. If neither valid, send -1.
        Mapping derived from self.classes order:
            0 bird, 1 cat, 2 llama, 3 rabbit, 4 deer, 5 dog, 6 snail, 7 swan

        OSC Path: /shadow_puppet/label_index
        Payload: int (index or -1)
        """
        try:
            if not self.numeric_client:
                return
            chosen = None
            if left_hand and left_hand in self.classes:
                chosen = left_hand
            elif right_hand and right_hand in self.classes:
                chosen = right_hand
            value = self.classes.index(chosen) if chosen else -1
            self.numeric_client.send_message("/shadow_puppet/label_index", value)
            logger.debug(f"Numeric label sent: {value} ({chosen if chosen else 'none'}) to port {self.numeric_label_port}")
        except Exception as e:
            logger.error(f"Failed to send numeric label index: {e}")

    def send_status_to_touchdesigner(self, status="detecting"):
        """Send status message to TouchDesigner."""
        try:
            self.osc_client.send_message("/shadow_puppet/status", status)
            if status == "no_hands":
                self.osc_client.send_message("/shadow_puppet/gesture", "none")
                self.osc_client.send_message("/shadow_puppet/confidence", 0.0)
        except Exception as e:
            logger.error(f"Failed to send status: {e}")

    def calculate_hand_position(self, landmarks):
        """Calculate normalized hand center position."""
        if not landmarks or not landmarks.landmarks:
            return None

        # Use wrist position as hand center
        wrist = landmarks.landmarks[0]  # Wrist is index 0

        # Normalize to 0-1 range (assuming 640x480 camera)
        normalized_x = wrist[0] / 640.0
        normalized_y = wrist[1] / 480.0

        return (normalized_x, normalized_y)

    def update_fps(self):
        """Update FPS calculation."""
        current_time = time.time()
        if current_time - self.last_fps_time >= 1.0:
            self.fps = self.frame_count / (current_time - self.last_fps_time)
            self.frame_count = 0
            self.last_fps_time = current_time

    def run_enhanced_bridge(self):
        """Run the enhanced TouchDesigner bridge with landmark streaming."""
        logger.info("Starting Enhanced MediaPipe TouchDesigner bridge...")
        logger.info("Streaming: Gestures + Landmarks + Hand Positions")

        # Open camera
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            logger.error("Could not open camera")
            return

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                self.frame_count += 1

                # Flip frame horizontally (mirror effect)
                frame = cv2.flip(frame, 1)

                # Predict both hands
                two_hand_detection, left_animal, left_confidence, right_animal, right_confidence = self.predict_both_hands(frame)

                if two_hand_detection and two_hand_detection.has_any_hand():
                    # Stream landmark data to TouchDesigner
                    if self.stream_landmarks:
                        self.send_landmark_data_to_touchdesigner(two_hand_detection)

                    # Update stable output buffer with gesture recognition
                    status = self.output_buffer.update(left_animal, left_confidence, right_animal, right_confidence)

                    # Send gesture recognition to TouchDesigner if confirmed
                    if status['should_send'] and status['confirmed_gesture']:
                        # Calculate hand positions
                        hand_positions = {}
                        if two_hand_detection.left_hand:
                            hand_positions['left'] = self.calculate_hand_position(two_hand_detection.left_hand)
                        if two_hand_detection.right_hand:
                            hand_positions['right'] = self.calculate_hand_position(two_hand_detection.right_hand)

                        # Send confirmed gesture to TouchDesigner
                        self.send_gesture_to_touchdesigner(
                            status['confirmed_gesture'],
                            status['confirmed_confidence'],
                            status['confirmed_left_hand'],
                            status['confirmed_right_hand'],
                            hand_positions
                        )

                        # Mark as sent
                        self.output_buffer.mark_sent()

                    # Visualize both hands
                    frame = self.extractor.visualize_both_hands(frame, two_hand_detection)

                    # Draw enhanced overlay
                    self.draw_enhanced_overlay(frame, two_hand_detection, left_animal, left_confidence, right_animal, right_confidence, status)

                else:
                    # No hands detected
                    self.send_status_to_touchdesigner("no_hands")
                    self.draw_no_hands_overlay(frame)

                # Update FPS
                self.update_fps()

                # Display frame
                cv2.imshow('Enhanced MediaPipe TouchDesigner Bridge', frame)

                # Handle key presses
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    # Reset buffer
                    from mediapipe_touchdesigner_bridge import TwoHandStableOutputBuffer
                    self.output_buffer = TwoHandStableOutputBuffer(
                        self.output_buffer.confidence_threshold,
                        self.output_buffer.stability_duration
                    )
                    logger.info("Output buffer reset")
                elif key == ord('l'):
                    # Toggle landmark streaming
                    self.stream_landmarks = not self.stream_landmarks
                    logger.info(f"Landmark streaming: {self.stream_landmarks}")
                elif key == ord('f'):
                    # Cycle landmark format
                    formats = ["individual", "array", "both"]
                    current_index = formats.index(self.landmark_format)
                    self.landmark_format = formats[(current_index + 1) % len(formats)]
                    logger.info(f"Landmark format: {self.landmark_format}")

        except KeyboardInterrupt:
            logger.info("Bridge interrupted by user")

        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.extractor.cleanup()
            logger.info("Enhanced TouchDesigner bridge stopped")

    def draw_enhanced_overlay(self, frame, two_hand_detection, left_animal, left_confidence, right_animal, right_confidence, status):
        """Draw enhanced overlay with landmark streaming info."""
        h, w = frame.shape[:2]

        # Background
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (w-10, 280), (0, 0, 0), -1)
        frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)

        # Title
        cv2.putText(frame, "Enhanced MediaPipe â†’ TouchDesigner",
                   (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

        # TouchDesigner output
        if status['confirmed_gesture']:
            cv2.putText(frame, f"TouchDesigner: {status['confirmed_gesture']}",
                       (20, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        else:
            cv2.putText(frame, "TouchDesigner: Detecting...",
                       (20, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)

        # Streaming status
        y_pos = 95
        streaming_info = []
        if self.stream_landmarks:
            streaming_info.append(f"Landmarks ({self.landmark_format})")
        streaming_info.append("Gestures")

        cv2.putText(frame, f"Streaming: {', '.join(streaming_info)}",
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 200, 255), 2)
        y_pos += 25

        # Hand detections
        cv2.putText(frame, f"Hands: {two_hand_detection.num_hands_detected}/2",
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        y_pos += 25

        # Left hand
        if left_animal:
            color = (255, 0, 0) if left_confidence >= self.output_buffer.confidence_threshold else (128, 0, 0)
            cv2.putText(frame, f"Left: {left_animal} ({left_confidence:.1%})",
                       (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        else:
            cv2.putText(frame, "Left: No hand",
                       (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 2)
        y_pos += 20

        # Right hand
        if right_animal:
            color = (0, 0, 255) if right_confidence >= self.output_buffer.confidence_threshold else (0, 0, 128)
            cv2.putText(frame, f"Right: {right_animal} ({right_confidence:.1%})",
                       (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        else:
            cv2.putText(frame, "Right: No hand",
                       (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 2)
        y_pos += 25

        # Status
        cv2.putText(frame, f"Status: {status['status_message']}",
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        y_pos += 20

        # Performance stats
        cv2.putText(frame, f"FPS: {self.fps:.1f} | Gestures: {self.predictions_sent} | Landmarks: {self.landmarks_sent}",
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180, 180, 180), 1)
        y_pos += 25

        # Controls
        cv2.putText(frame, "Controls: 'q'=quit, 'r'=reset, 'l'=toggle landmarks, 'f'=format",
                   (20, h-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    def draw_no_hands_overlay(self, frame):
        """Draw overlay when no hands are detected."""
        h, w = frame.shape[:2]

        cv2.putText(frame, "No hands detected",
                   (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
        cv2.putText(frame, "Show your hands to start detection",
                   (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

def main():
    """Main enhanced bridge function."""
    print("Enhanced MediaPipe TouchDesigner Bridge")
    print("=" * 45)
    print("Streams gesture recognition AND raw landmark data")
    print("Optimized for TouchDesigner integration")

    try:
        # Configuration for live demo
        td_ip = "127.0.0.1"
        td_port = 7000
        confidence_threshold = 0.7    # Higher for demo stability
        stability_duration = 1.0      # Faster response for demo
        stream_landmarks = True       # Enable landmark streaming
        stream_video = False          # Video streaming (future feature)

        print(f"\nConfiguration:")
        print(f"TouchDesigner: {td_ip}:{td_port}")
        print(f"Confidence threshold: {confidence_threshold:.0%}")
        print(f"Stability duration: {stability_duration:.1f}s")
        print(f"Landmark streaming: {stream_landmarks}")

        bridge = EnhancedTouchDesignerBridge(
            td_ip, td_port, confidence_threshold, stability_duration,
            stream_landmarks, stream_video
        )

        bridge.run_enhanced_bridge()

        print("\n[SUCCESS] Enhanced TouchDesigner bridge completed!")

    except Exception as e:
        print(f"[ERROR] Enhanced bridge failed: {e}")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())
